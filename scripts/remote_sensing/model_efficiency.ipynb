{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c16bc3c-7647-402d-8f15-7f111ddb85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from torchvision.models import efficientnet_b3\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11e41e7-cd05-4793-8eac-6ef4d5e7adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "class EfficientNetB3(nn.Module):\n",
    "  def __init__(self, num_classes, stages):\n",
    "    super(EfficientNetB3, self).__init__()\n",
    "    self.model = efficientnet_b3(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "    self.model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True), \n",
    "        nn.Linear(in_features=1536, out_features=num_classes, bias=True)\n",
    "    )\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # qse in stage 2\n",
    "    if 2 in stages:\n",
    "        self.model.features[1][0].block[1] = SqueezeExcitation(40,10)\n",
    "        self.model.features[1][1].block[1] = SqueezeExcitation(24,6)\n",
    "    # qse in stage 3\n",
    "    if 3 in stages:\n",
    "        self.model.features[2][0].block[2] = SqueezeExcitation(144,6)\n",
    "        for i in range(1,3):\n",
    "          self.model.features[2][1].block[2] = SqueezeExcitation(192,8)\n",
    "    # qse in stage 4\n",
    "    if 4 in stages:\n",
    "      self.model.features[3][0].block[2] = SqueezeExcitation(192,8)\n",
    "      for i in range(1,3):  \n",
    "        self.model.features[3][i].block[2] = SqueezeExcitation(288,12)\n",
    "    # qse in stage 5\n",
    "    if 5 in stages:\n",
    "      self.model.features[4][0].block[2] = SqueezeExcitation(288,12)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[4][i].block[2] = SqueezeExcitation(576,24)\n",
    "    # qse in stage 6\n",
    "    if 6 in stages:\n",
    "      self.model.features[5][0].block[2] = SqueezeExcitation(576,24)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[5][i].block[2] = SqueezeExcitation(816,34)\n",
    "    # qse in stage 7\n",
    "    if 7 in stages:\n",
    "      self.model.features[6][0].block[2] = SqueezeExcitation(816,34)\n",
    "      for i in range(1,6):\n",
    "        self.model.features[6][i].block[2] = SqueezeExcitation(1392,58)\n",
    "    # qse in stage 8\n",
    "    if 8 in stages:\n",
    "      self.model.features[7][0].block[2] = SqueezeExcitation(1392,58)\n",
    "      self.model.features[7][1].block[2] = SqueezeExcitation(2304,96)\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdaa2d1b-b97a-4343-a1b4-c922d1aa87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAM\n",
    "class QAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_dim):\n",
    "        super(QAM, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((2, 2)),\n",
    "            nn.Conv2d(in_channels, reduction_dim, kernel_size=1, stride=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduction_dim, in_channels, kernel_size=1, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        x = self.fc(x)\n",
    "\n",
    "        residual[:, :, :h//2, :w//2].mul_(x[:, :, 0:1, 0:1])  # Top-left\n",
    "        residual[:, :, :h//2, w//2:].mul_(x[:, :, 0:1, 1:2])  # Top-right\n",
    "        residual[:, :, h//2:, :w//2].mul_(x[:, :, 1:2, 0:1])  # Bottom-left\n",
    "        residual[:, :, h//2:, w//2:].mul_(x[:, :, 1:2, 1:2])  # Bottom-right\n",
    "\n",
    "        return residual\n",
    "class EfficientNetB3QAM(nn.Module):\n",
    "  def __init__(self, num_classes, stages):\n",
    "    super(EfficientNetB3QAM, self).__init__()\n",
    "    self.model = efficientnet_b3(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "    self.model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True), \n",
    "        nn.Linear(in_features=1536, out_features=num_classes, bias=True)\n",
    "    )\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # qse in stage 2\n",
    "    if 2 in stages:\n",
    "        self.model.features[1][0].block[1] = QAM(40,10)\n",
    "        self.model.features[1][1].block[1] = QAM(24,6)\n",
    "    # qse in stage 3\n",
    "    if 3 in stages:\n",
    "        self.model.features[2][0].block[2] = QAM(144,6)\n",
    "        for i in range(1,3):\n",
    "          self.model.features[2][1].block[2] = QAM(192,8)\n",
    "    # qse in stage 4\n",
    "    if 4 in stages:\n",
    "      self.model.features[3][0].block[2] = QAM(192,8)\n",
    "      for i in range(1,3):  \n",
    "        self.model.features[3][i].block[2] = QAM(288,12)\n",
    "    # qse in stage 5\n",
    "    if 5 in stages:\n",
    "      self.model.features[4][0].block[2] = QAM(288,12)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[4][i].block[2] = QAM(576,24)\n",
    "    # qse in stage 6\n",
    "    if 6 in stages:\n",
    "      self.model.features[5][0].block[2] = QAM(576,24)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[5][i].block[2] = QAM(816,34)\n",
    "    # qse in stage 7\n",
    "    if 7 in stages:\n",
    "      self.model.features[6][0].block[2] = QAM(816,34)\n",
    "      for i in range(1,6):\n",
    "        self.model.features[6][i].block[2] = QAM(1392,58)\n",
    "    # qse in stage 8\n",
    "    if 8 in stages:\n",
    "      self.model.features[7][0].block[2] = QAM(1392,58)\n",
    "      self.model.features[7][1].block[2] = QAM(2304,96)\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3968f7-2f6a-42ee-a877-37004b7d008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAM\n",
    "class HAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_dim):\n",
    "        super(HAM, self).__init__()\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.quadrant_avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, reduction_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduction_dim, in_channels, 1),\n",
    "        )\n",
    "        self.scale_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # Global Average Pooling (GAP) and Quadrant Average Pooling (QAP)\n",
    "        gap_out = self.fc(self.global_avgpool(x))\n",
    "        qap_out = self.fc(self.quadrant_avgpool(x))\n",
    "        \n",
    "        # Applying element-wise multiplication using scaling activations\n",
    "        x = self.scale_activation(self.scale_activation(gap_out) * qap_out)\n",
    "        \n",
    "        # Apply quadrant-specific scaling using in-place multiplication (mul_)\n",
    "        residual[:, :, 0:h//2, 0:w//2].mul_(x[:, :, 0:1, 0:1])  # Top-left\n",
    "        residual[:, :, 0:h//2, w//2:].mul_(x[:, :, 0:1, 1:2])  # Top-right\n",
    "        residual[:, :, h//2:, 0:w//2].mul_(x[:, :, 1:2, 0:1])  # Bottom-left\n",
    "        residual[:, :, h//2:, w//2:].mul_(x[:, :, 1:2, 1:2])  # Bottom-right\n",
    "\n",
    "        return residual\n",
    "\n",
    "class EfficientNetB3HAM(nn.Module):\n",
    "  def __init__(self, num_classes, stages):\n",
    "    super(EfficientNetB3HAM, self).__init__()\n",
    "    self.model = efficientnet_b3(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "    self.model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),  # Dropout layer with 30% dropout rate\n",
    "        nn.Linear(in_features=1536, out_features=num_classes, bias=True)  # Adjust in_features to match EfficientNetB3 output\n",
    "    )\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # qse in stage 2\n",
    "    if 2 in stages:\n",
    "        self.model.features[1][0].block[1] = HAM(40,10)\n",
    "        self.model.features[1][1].block[1] = HAM(24,6)\n",
    "    # qse in stage 3\n",
    "    if 3 in stages:\n",
    "        self.model.features[2][0].block[2] = HAM(144,6)\n",
    "        for i in range(1,3):\n",
    "          self.model.features[2][1].block[2] = HAM(192,8)\n",
    "    # qse in stage 4\n",
    "    if 4 in stages:\n",
    "      self.model.features[3][0].block[2] = HAM(192,8)\n",
    "      for i in range(1,3):  \n",
    "        self.model.features[3][i].block[2] = HAM(288,12)\n",
    "    # qse in stage 5\n",
    "    if 5 in stages:\n",
    "      self.model.features[4][0].block[2] = HAM(288,12)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[4][i].block[2] = HAM(576,24)\n",
    "    # qse in stage 6\n",
    "    if 6 in stages:\n",
    "      self.model.features[5][0].block[2] = HAM(576,24)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[5][i].block[2] = HAM(816,34)\n",
    "    # qse in stage 7\n",
    "    if 7 in stages:\n",
    "      self.model.features[6][0].block[2] = HAM(816,34)\n",
    "      for i in range(1,6):\n",
    "        self.model.features[6][i].block[2] = HAM(1392,58)\n",
    "    # qse in stage 8\n",
    "    if 8 in stages:\n",
    "      self.model.features[7][0].block[2] = HAM(1392,58)\n",
    "      self.model.features[7][1].block[2] = HAM(2304,96)\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c5c818-494c-4f6c-97fe-f285fd16bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://github.com/Peachypie98/CBAM/blob/main/cbam.py\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r=16):\n",
    "        super(CAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels//16, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels//16, out_features=self.channels, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b,c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b,c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        return torch.sigmoid(output) * x\n",
    "class EfficientNetB3CAM(nn.Module):\n",
    "  def __init__(self, num_classes, stages):\n",
    "    super(EfficientNetB3CAM, self).__init__()\n",
    "    self.model = efficientnet_b3(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "    self.model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),  # Dropout layer with 30% dropout rate\n",
    "        nn.Linear(in_features=1536, out_features=num_classes, bias=True)  # Adjust in_features to match EfficientNetB3 output\n",
    "    )\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # CBAM in stage 2\n",
    "    if 2 in stages:\n",
    "        self.model.features[1][0].block[1] = CAM(40,10)\n",
    "        self.model.features[1][1].block[1] = CAM(24,6)\n",
    "    # qse in stage 3\n",
    "    if 3 in stages:\n",
    "        self.model.features[2][0].block[2] = CAM(144,6)\n",
    "        for i in range(1,3):\n",
    "          self.model.features[2][1].block[2] = CAM(192,8)\n",
    "    # qse in stage 4\n",
    "    if 4 in stages:\n",
    "      self.model.features[3][0].block[2] = CAM(192,8)\n",
    "      for i in range(1,3):  \n",
    "        self.model.features[3][i].block[2] = CAM(288,12)\n",
    "    # qse in stage 5\n",
    "    if 5 in stages:\n",
    "      self.model.features[4][0].block[2] = CAM(288,12)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[4][i].block[2] = CAM(576,24)\n",
    "    # qse in stage 6\n",
    "    if 6 in stages:\n",
    "      self.model.features[5][0].block[2] = CAM(576,24)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[5][i].block[2] = CAM(816,34)\n",
    "    # qse in stage 7\n",
    "    if 7 in stages:\n",
    "      self.model.features[6][0].block[2] = CAM(816,34)\n",
    "      for i in range(1,6):\n",
    "        self.model.features[6][i].block[2] = CAM(1392,58)\n",
    "    # qse in stage 8\n",
    "    if 8 in stages:\n",
    "      self.model.features[7][0].block[2] = CAM(1392,58)\n",
    "      self.model.features[7][1].block[2] = CAM(2304,96)\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abecad73-e20a-41c4-b2e3-35d66b1af4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA(nn.Module):\n",
    "    def __init__(self, channels, b=1, gamma=2):\n",
    "        super(ECA, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channels = channels\n",
    "        self.b = b\n",
    "        self.gamma = gamma\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=self.kernel_size(), padding=(self.kernel_size() - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def kernel_size(self):\n",
    "        k = int(abs((math.log2(self.channels)/self.gamma)+ self.b/self.gamma))\n",
    "        out = k if k % 2 else k+1\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        y = self.sigmoid(y)\n",
    "        return x * y.expand_as(x)\n",
    "class EfficientNetB3ECA(nn.Module):\n",
    "  def __init__(self, num_classes, stages):\n",
    "    super(EfficientNetB3ECA, self).__init__()\n",
    "    self.model = efficientnet_b3(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "    self.model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True), \n",
    "        nn.Linear(in_features=1536, out_features=num_classes, bias=True)\n",
    "    )\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # qse in stage 2\n",
    "    if 2 in stages:\n",
    "        self.model.features[1][0].block[1] = ECA(40)\n",
    "        self.model.features[1][1].block[1] = ECA(24)\n",
    "    # qse in stage 3\n",
    "    if 3 in stages:\n",
    "        self.model.features[2][0].block[2] = ECA(144)\n",
    "        for i in range(1,3):\n",
    "          self.model.features[2][1].block[2] = ECA(192)\n",
    "    # qse in stage 4\n",
    "    if 4 in stages:\n",
    "      self.model.features[3][0].block[2] = ECA(192)\n",
    "      for i in range(1,3):  \n",
    "        self.model.features[3][i].block[2] = ECA(288)\n",
    "    # qse in stage 5\n",
    "    if 5 in stages:\n",
    "      self.model.features[4][0].block[2] = ECA(288)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[4][i].block[2] = ECA(576)\n",
    "    # qse in stage 6\n",
    "    if 6 in stages:\n",
    "      self.model.features[5][0].block[2] = ECA(576)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[5][i].block[2] = ECA(816)\n",
    "    # qse in stage 7\n",
    "    if 7 in stages:\n",
    "      self.model.features[6][0].block[2] = ECA(816)\n",
    "      for i in range(1,6):\n",
    "        self.model.features[6][i].block[2] = ECA(1392)\n",
    "    # qse in stage 8\n",
    "    if 8 in stages:\n",
    "      self.model.features[7][0].block[2] = ECA(1392)\n",
    "      self.model.features[7][1].block[2] = ECA(2304)\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e573f4-ae09-47a0-9f9e-dfa95eb3a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "# remove the max pooling idk why it is affecting the accuracy of both the cam and the sam\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=True):\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=7, stride=1, padding=3, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool = torch.mean(x, 1, keepdim=True)\n",
    "        output = self.conv(avg_pool)\n",
    "        output = torch.sigmoid(output) * x  \n",
    "        return output\n",
    "\n",
    "\n",
    "class SESAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SESAM, self).__init__()\n",
    "        self.se = SqueezeExcitation(in_channels, reduced_dim)\n",
    "        self.sam = SAM()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.se(x)\n",
    "        x = self.sam(x)\n",
    "        return x\n",
    "\n",
    "class EfficientNetB3SESAM(nn.Module):\n",
    "  def __init__(self, num_classes, stages):\n",
    "    super(EfficientNetB3SESAM, self).__init__()\n",
    "    self.model = efficientnet_b3(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "    self.model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True), \n",
    "        nn.Linear(in_features=1536, out_features=num_classes, bias=True)\n",
    "    )\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # qse in stage 2\n",
    "    if 2 in stages:\n",
    "        self.model.features[1][0].block[1] = SESAM(40,10)\n",
    "        self.model.features[1][1].block[1] = SESAM(24,6)\n",
    "    # qse in stage 3\n",
    "    if 3 in stages:\n",
    "        self.model.features[2][0].block[2] = SESAM(144,6)\n",
    "        for i in range(1,3):\n",
    "          self.model.features[2][1].block[2] = SESAM(192,8)\n",
    "    # qse in stage 4\n",
    "    if 4 in stages:\n",
    "      self.model.features[3][0].block[2] = SESAM(192,8)\n",
    "      for i in range(1,3):  \n",
    "        self.model.features[3][i].block[2] = SESAM(288,12)\n",
    "    # qse in stage 5\n",
    "    if 5 in stages:\n",
    "      self.model.features[4][0].block[2] = SESAM(288,12)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[4][i].block[2] = SESAM(576,24)\n",
    "    # qse in stage 6\n",
    "    if 6 in stages:\n",
    "      self.model.features[5][0].block[2] = SESAM(576,24)\n",
    "      for i in range(1,5):\n",
    "        self.model.features[5][i].block[2] = SESAM(816,34)\n",
    "    # qse in stage 7\n",
    "    if 7 in stages:\n",
    "      self.model.features[6][0].block[2] = SESAM(816,34)\n",
    "      for i in range(1,6):\n",
    "        self.model.features[6][i].block[2] = SESAM(1392,58)\n",
    "    # qse in stage 8\n",
    "    if 8 in stages:\n",
    "      self.model.features[7][0].block[2] = SESAM(1392,58)\n",
    "      self.model.features[7][1].block[2] = SESAM(2304,96)\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed572a3-378b-4a40-a85b-6181485a3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_of_error(data, confidence=0.95):\n",
    "    data = np.array(data)\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        raise ValueError(\"At least two data points are required to calculate margin of error.\")\n",
    "    \n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "    z_score = stats.norm.ppf(1 - (1 - confidence) / 2)  # 1.96 for 95% CI\n",
    "    \n",
    "    moe = z_score * (std_dev / np.sqrt(n))\n",
    "    return mean, moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369211ea-b591-4f18-9349-d991db5007fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1000\n",
    "stages = [2,3,4,5,6,7,8]\n",
    "model_baseline = EfficientNetB3(num_classes, stages).eval().cuda()\n",
    "model_qam = EfficientNetB3QAM(num_classes, stages).eval().cuda()\n",
    "model_ham = EfficientNetB3HAM(num_classes, stages).eval().cuda()\n",
    "model_cam = EfficientNetB3CAM(num_classes, stages).eval().cuda()\n",
    "model_eca = EfficientNetB3ECA(num_classes, stages).eval().cuda()\n",
    "model_sesam = EfficientNetB3SESAM(num_classes, stages).eval().cuda()\n",
    "models = {\"baseline\": model_baseline,\n",
    "         \"qam\": model_qam,\n",
    "         \"ham\": model_ham,\n",
    "         \"cam\": model_cam,\n",
    "         \"eca\": model_eca,\n",
    "         \"sesam\": model_sesam   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f499f81-e427-447c-a12c-4565adbaeee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "1330719104.0\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "1336346048.0\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "1343813760.0\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "1328818272.0\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "1328962368.0\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "1333027200.0\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "input_single = torch.randn(1, 3, 256, 256).cuda()\n",
    "input_batch = torch.randn(batch_size, 3, 224, 224).cuda()\n",
    "df = pd.DataFrame(columns=[\"Model\", \"FLOPs\", \"Parameters\", \"Throughput\", \"Margin of Error\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "throughputs = {\"baseline\":[], \"qam\":[], \"ham\":[], \"cam\":[], \"eca\":[], \"sesam\":[]}\n",
    "for index, (key, value) in enumerate(models.items()):\n",
    "    flops, params = profile(value, inputs=(input_single,))\n",
    "    print(flops)\n",
    "    for i in range(100):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = value(input_batch)\n",
    "        end = time.time()\n",
    "        throughputs[key].append(batch_size/(end-start))\n",
    "    mean, moe = margin_of_error(throughputs[key])\n",
    "    df.loc[len(df)] = {\"Model\": key, \"FLOPs\": flops, \"Parameters\": params, \"Throughput\": mean, \"Margin of Error\":moe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4317d6d2-877c-48fa-ac80-8b5d1ec59310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Margin of Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1.330719e+09</td>\n",
       "      <td>12233232.0</td>\n",
       "      <td>102.421952</td>\n",
       "      <td>0.218486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qam</td>\n",
       "      <td>1.336346e+09</td>\n",
       "      <td>12233232.0</td>\n",
       "      <td>81.549878</td>\n",
       "      <td>0.438770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>1.343814e+09</td>\n",
       "      <td>12233232.0</td>\n",
       "      <td>63.544629</td>\n",
       "      <td>0.199166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cam</td>\n",
       "      <td>1.328818e+09</td>\n",
       "      <td>13160479.0</td>\n",
       "      <td>82.972892</td>\n",
       "      <td>0.129246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eca</td>\n",
       "      <td>1.328962e+09</td>\n",
       "      <td>10356899.0</td>\n",
       "      <td>109.155222</td>\n",
       "      <td>0.607856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sesam</td>\n",
       "      <td>1.333027e+09</td>\n",
       "      <td>12234482.0</td>\n",
       "      <td>85.150951</td>\n",
       "      <td>0.150591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model         FLOPs  Parameters  Throughput  Margin of Error\n",
       "0  baseline  1.330719e+09  12233232.0  102.421952         0.218486\n",
       "1       qam  1.336346e+09  12233232.0   81.549878         0.438770\n",
       "2       ham  1.343814e+09  12233232.0   63.544629         0.199166\n",
       "3       cam  1.328818e+09  13160479.0   82.972892         0.129246\n",
       "4       eca  1.328962e+09  10356899.0  109.155222         0.607856\n",
       "5     sesam  1.333027e+09  12234482.0   85.150951         0.150591"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e00a2d-c172-4891-b072-871d97a2493e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-test] *",
   "language": "python",
   "name": "conda-env-.conda-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
